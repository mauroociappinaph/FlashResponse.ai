# FlashResponse.ai ‚ö°

> **Low-Latency AI Inference Demo**
>
> Una prueba de concepto de ingenier√≠a enfocada en minimizar el TTFT (Time-To-First-Token) utilizando Google Gemini 2.0 Flash-Lite y arquitecturas reactivas.

![Status](https://img.shields.io/badge/Status-MVP_Ready-success)
![Model](https://img.shields.io/badge/AI_Model-Gemini_2.0_Flash_Lite-blue)
![Tech](https://img.shields.io/badge/Stack-React_19_+_Tailwind-38bdf8)

## üéØ Objetivo del Proyecto

El objetivo de **FlashResponse.ai** no es construir otro chatbot gen√©rico, sino demostrar c√≥mo integrar Inteligencia Artificial en productos de producci√≥n donde la **latencia** es un KPI cr√≠tico.

Este proyecto resuelve el problema de la "fricci√≥n cognitiva" en interfaces de IA, reduciendo el tiempo de respuesta inicial de ~1.5s (promedio en modelos Pro) a **<500ms**, creando una experiencia de usuario fluida e instant√°nea.

## üöÄ Caracter√≠sticas Principales

*   **Arquitectura Low-Latency:** Optimizada para un *Time-To-First-Token* (TTFT) inferior a 500ms.
*   **Streaming Nativo:** Renderizado de tokens en tiempo real para feedback visual inmediato.
*   **Observabilidad Integrada:** Dashboard de m√©tricas en vivo en cada mensaje:
    *   `TTFT`: Latencia de red + inferencia inicial.
    *   `Total Time`: Duraci√≥n completa de la generaci√≥n.
    *   `TPS`: Tokens por segundo (Throughput).
*   **Multimodalidad:** Procesamiento de im√°genes y texto sin sacrificar velocidad.
*   **Ingenier√≠a de Prompts (System Instructions):** "Persona" inyectada para respuestas t√©cnicas, concisas y con *Context Grounding* (fecha/hora real).

## üõ†Ô∏è Stack Tecnol√≥gico

*   **Frontend:** React 19 + TypeScript + Vite.
*   **Estilos:** Tailwind CSS (Dise√±o moderno, Dark Mode por defecto).
*   **AI SDK:** `@google/genai` (Google GenAI SDK para Web).
*   **Modelo:** `gemini-2.0-flash-lite-preview-02-05` (Elegido por su balance costo/velocidad).
*   **Routing:** React Router DOM v7.

## üìê Arquitectura

El sistema sigue un patr√≥n directo **Client-to-Model** para minimizar saltos de red en esta demo (Edge-ready):

1.  **Input:** El usuario env√≠a texto o imagen.
2.  **Pre-processing:** React gestiona el estado y prepara el payload multimodal.
3.  **Inference:** Llamada directa a Gemini Flash-Lite v√≠a SDK.
4.  **Streaming:** Los chunks de respuesta se procesan y renderizan al vuelo.
5.  **Metrics:** Un `PerformanceObserver` interno calcula los tiempos entre el request y el primer byte.

## üì¶ Instalaci√≥n y Uso

1.  **Clonar el repositorio:**
    ```bash
    git clone https://github.com/tu-usuario/flash-response-ai.git
    cd flash-response-ai
    ```

2.  **Instalar dependencias:**
    ```bash
    npm install
    ```

3.  **Configurar Variables de Entorno:**
    Crea un archivo `.env` en la ra√≠z (o configura tu entorno):
    ```env
    API_KEY=tu_google_api_key_aqui
    ```

4.  **Iniciar en desarrollo:**
    ```bash
    npm run start
    ```

## üß† Decisiones de Ingenier√≠a

### ¬øPor qu√© Flash-Lite?
Para casos de uso de alta frecuencia (Customer Support, Data Extraction), la "inteligencia profunda" de un modelo Pro a menudo es innecesaria y costosa. Flash-Lite ofrece un *throughput* masivo a una fracci√≥n del costo y latencia, ideal para sistemas *Real-Time*.

### UX Optimista
La interfaz implementa estados de carga inmediatos y feedback visual durante el streaming. No esperamos a que la respuesta est√© completa; mostramos la "intenci√≥n" de la IA instant√°neamente.

### Context Grounding
Para evitar alucinaciones temporales, el sistema inyecta din√°micamente la fecha y hora del navegador en el `System Instruction` antes de cada sesi√≥n, anclando al modelo en el presente.

---

**Autor:** Tu Nombre
**Licencia:** MIT
